# -*- coding: utf-8 -*-
"""ANN Model for Optimization

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n78snUwTqMSOmL7Z9sWe6wJs4Zph9qtn
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neural_network import MLPRegressor
from skopt import gp_minimize
from skopt.space import Real
from skopt.utils import use_named_args
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
file_path = "278 Short String GL optimized.xlsx"
df = pd.read_excel(file_path)

# Drop rows with missing values
df.dropna(inplace=True)

# Strip column names of extra spaces
df.columns = df.columns.str.strip()

# Fill missing values with column means
df.fillna(df.mean(), inplace=True)

# Define feature variables and target variable
feature_columns = ["WHTP", "DSP", "Choke", "THP", "Pressure DS Choke",
                    "CHP", "TAP", "BCP"]
target_column = "OIL (BOPD)"

df['Time'] = range(len(df))  # Adding time as a feature
feature_columns.append('Time')

X = df[feature_columns]
y = df[target_column]

# Drop features where min == max (constant values)
variable_features = [col for col in feature_columns if X[col].min() < X[col].max()]
X = X[variable_features]  # Keep only variable features

# Train an Artificial Neural Network (ANN) model
ann_model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)
ann_model.fit(X, y)

# Define realistic target increase (above max observed production over 4 months)
target_increase = 1.2 * max(y)  # Ensure more than the maximum observed

# Define the optimization function using Bayesian Optimization
space = [Real(X[col].min(), X[col].max(), name=col) for col in variable_features]

@use_named_args(space)
def objective(**params):
    params_df = pd.DataFrame([params])
    predicted_value = ann_model.predict(params_df)[0]
    return abs(predicted_value - target_increase)  # Penalize deviation from realistic target

# Run Bayesian Optimization
res = gp_minimize(objective, space, n_calls=50, random_state=42)

# Extract optimized values
optimized_values = {variable_features[i]: res.x[i] for i in range(len(variable_features))}
optimized_oil_production = ann_model.predict(pd.DataFrame([optimized_values]))[0]

# Ensure optimization is time-aligned (targeting higher production at later time steps)
optimized_values['Time'] = max(df['Time'])  # Set time to latest available time step

# Create DataFrame with optimized values
optimized_df = pd.DataFrame([optimized_values])
optimized_df["Predicted_OIL_BOPD"] = optimized_oil_production

# Save optimized results to CSV
optimized_csv_path = "optimized_oil_production.csv"
optimized_df.to_csv(optimized_csv_path, index=False)

# Evaluate model performance
y_pred = ann_model.predict(X)
r2 = r2_score(y, y_pred)
print("R2 Score:", r2)


# Feature Importance Analysis (Using Permutation Importance)
from sklearn.inspection import permutation_importance
perm_importance = permutation_importance(ann_model, X, y, scoring='r2')
importance_df = pd.DataFrame({'Feature': variable_features, 'Importance': perm_importance.importances_mean})
importance_df.sort_values(by='Importance', ascending=False, inplace=True)

# Comparison Plot - Normal vs. Optimized Production
plt.figure(figsize=(6,4))
plt.bar(['Normal Production', 'Optimized Production'], [max(y), optimized_oil_production], color=['blue', 'green'])
plt.ylabel('Oil Production (BOPD)')
plt.title('Comparison of Normal vs Optimized Production')
plt.show()

# Scatter Plot of Predictions vs. Actual
plt.figure(figsize=(6,4))
sns.scatterplot(x=y, y=y_pred)
plt.xlabel('Actual Production')
plt.ylabel('Predicted Production')
plt.title('Actual vs. Predicted Oil Production')
plt.show()

print("Optimized Values:", optimized_values)
print("Predicted Oil Production:", optimized_oil_production)
print("CSV file saved at:", optimized_csv_path)